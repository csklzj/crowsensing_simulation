卷积神经网络-Stride[步幅]
如下使用3x3的过滤器卷积7x7的图像，步幅即Stride设置成了2，即上下左右移动步长均是2.

2 3 7 4 6 2 9
6 6 9 8 7 4 3
3 4 8 3 8 9 7      3 4 1    91 100 83
7 8 3 6 6 3 4  *   1 0 2  = 69 91 127
4 2 1 8 3 4 6     -1 0 3    44 72 74
3 2 4 1 9 8 3
0 1 3 9 2 1 4

当在nxn的矩阵中使用filter fxf，padding p, stride s时，可以计算出最后的矩阵维度
  n+2p-f          n+2p-f
  ------  + 1  x  ------ + 1,注意其中除不尽时均取下整数
     s               s




卷积神经网络-padding[像素填充]

前言：使用不带填充的卷积神经网络，卷积之后的矩阵会非常小；输入矩阵的边缘像素只被计算过一次，而中间像素被卷积多次
表示丢失图像角落信息；为了解决这个问题，就多图像进行padding，即像素填充

             0 0 0 0 0 0
1 2 3 4      0 1 2 3 4 0
5 6 7 8      0 5 6 7 8 0      此时的padding = 1
9 1 2 3      0 9 1 2 3 0
5 6 7 8      0 5 6 7 8 0
             0 0 0 0 0 0

当使用padding后的图像进行卷积，可以削落边缘信息丢失的缺点

valid和same两种卷积方式
valid：不填充 nxn * fxf -> n-f+1 x n-f+1
same: 输入大小和输出大小相同，n+2p-f+1 x n+2p-f+1 -> nxn,即p=(f-1)/2
     有same可以知道，滤波器filter矩阵通常都是奇数，可能是由于奇数f使得p填充时对称；
     奇数f使滤波器矩阵有中心像素点。

深度学习-dropout
    dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定概率将其暂时从网络中丢弃。这只是暂时的，
    对于随机梯度下降来讲，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。
    dropout是CNN中防止过拟合提高效率的方式，究其原因如下：
    1.组合派
        大规模的神经网络有两个缺点：费时和容易过拟合
        解决过拟合一般采用的是ensemble方式，即训练多个模型做组合，但是费时问题就会变得更加重要了
        其中Dropout出现很好的可以解决这个问题，每次做完dropout，相当于从原始的网络中找到一个更瘦的网络
        动机论
            dropout强迫一个神经元和随机变量挑选出来的其他神经单元共同工作，达到好的效果。消除减弱了神经元节点间的联合适应性
            增强了泛化能力。
    2.噪声派
        对于每一个dropout后的网络，进行训练时，相当于做了Data Augmentation,因为总可以找到一个样本，使得在原始的网络中也能
        达到dropout之后的效果。
深度学习-Batch Normalization
1.Batch Normalization,批标准化和普通的数据标准化类似，是将分散的数据统一的一种做法，也是优化神经网络的
一种方式。
在神经网络中，数据分布对训练会产生影响。比如神经元x的值为1，某个Weights的初始值为0.1，这样后一层神经元计算结果就是Wx=0.1
当x=20的时候，Wx的结果就变成了2。在加上了一层激励函数，激活这个Wx值的时候，激活值就变成了~0.1和~1，接近与1的部分已经处在
激励函数的饱和阶段，也就是说,x无论再怎么扩大，tanh激励函数输出值也是接近1，这样来讲20++和20是一样的。这样的话，激励函数失去
了对输入函数的敏感性。
2.BN添加的位置，Batch normalization的batch是把数据分成小批，让小批数据进行stochastic gradient descent，而且在每批数据
进行前向传递时，对每一层都进行归一化处理。



